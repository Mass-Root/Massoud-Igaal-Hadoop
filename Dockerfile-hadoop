FROM debian:11

RUN apt-get update && apt-get install -y --no-install-recommends \
openjdk-11-jdk \
net-tools \
curl \
netcat \
gnupg \
libsnappy-dev \
wget \
nano \
net-tools \
openssh-server \
inetutils-ping \
python3 \
python3-setuptools \
python3-pip \
&& apt-get clean \
&& rm -rf /var/lib/apt/lists/*

RUN update-alternatives --display java

ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/
ENV PATH="${JAVA_HOME}/bin:${PATH}"


# --------------------------------------------------------
# JAVA
# --------------------------------------------------------


# RUN add-apt-repository ppa:openjdk-r/ppa
# RUN apt update
# RUN apt install -y --no-install-recommends \
#     openjdk-8-jdk
# For AMD based architecture use
# ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/
#COPY --from=jdk /usr/local/openjdk-11 /usr/lib/jvm/java-11-openjdk-arm64/
#ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-arm64/

# --------------------------------------------------------
# HADOOP
# --------------------------------------------------------
ENV HADOOP_VERSION=3.3.6
ENV HADOOP_URL=https://downloads.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz


RUN set -x \
    && curl -fSL "$HADOOP_URL" -o /tmp/hadoop.tar.gz \
    && tar -xvf /tmp/hadoop.tar.gz -C /opt/ \
    && rm /tmp/hadoop.tar.gz*

RUN ln -s /opt/hadoop-$HADOOP_VERSION/etc/hadoop /etc/hadoop
RUN mkdir /opt/hadoop-$HADOOP_VERSION/logs
RUN mkdir /hadoop-data

#ENV HADOOP_PREFIX=/opt/hadoop-$HADOOP_VERSION
ENV HADOOP_CONF_DIR=/etc/hadoop
ENV MULTIHOMED_NETWORK=1
ENV USER=root
ENV HADOOP_HOME=/opt/hadoop-$HADOOP_VERSION
# ENV PATH $HADOOP_PREFIX/bin/:$PATH
ENV PATH=$HADOOP_HOME/bin:$PATH
ENV PATH=$HADOOP_HOME/sbin:$PATH
# ENV PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH je doute que ça marche
ENV LANG C.UTF-8
ENV LC_ALL C.UTF-8

# Configurer SSH pour root
RUN mkdir /root/.ssh && \
    chmod 700 /root/.ssh && \
    echo "PermitRootLogin yes" >> /etc/ssh/sshd_config && \
    echo "root:root" | chpasswd

# Générer une paire de clés SSH pour root sans mot de passe et ajouter la clé publique aux authorized_keys
RUN ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa && \
    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys && \
    chmod 600 /root/.ssh/authorized_keys


USER root

RUN echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# Ajouter les variables d'environnement pour les utilisateurs Hadoop
RUN echo "export HDFS_NAMENODE_USER=root" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh && \
    echo "export HDFS_DATANODE_USER=root" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh && \
    echo "export HDFS_SECONDARYNAMENODE_USER=root" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh && \
    echo 'export YARN_RESOURCEMANAGER_USER=root' >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh && \
    echo 'export YARN_NODEMANAGER_USER=root' >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh


ADD entrypoint.sh /entrypoint.sh
RUN chmod a+x /entrypoint.sh

COPY conf/core-site.xml $HADOOP_CONF_DIR/core-site.xml
COPY conf/hdfs-site.xml $HADOOP_CONF_DIR/hdfs-site.xml
COPY conf/mapred-site.xml $HADOOP_CONF_DIR/mapred-site.xml
COPY conf/yarn-site.xml $HADOOP_CONF_DIR/yarn-site.xml

# ADD hadoop.env /hadoop.env

# RUN set -x && cd / && ./entrypoint.sh

# --------------------------------------------------------
# SPARK
# --------------------------------------------------------

ENV SPARK_VERSION spark-3.5.4
ENV SPARK_URL https://dlcdn.apache.org/spark/${SPARK_VERSION}/${SPARK_VERSION}-bin-hadoop3.tgz
ENV SPARK_HOME=/opt/$SPARK_VERSION
ENV PATH $SPARK_HOME/bin:$PATH
# ENV HADOOP_CONF_DIR=$SPARK_HOME/conf
ENV PYSPARK_PYTHON=python3
ENV PYTHONHASHSEED=1

RUN set -x \
    && curl -fSL "${SPARK_URL}" -o /tmp/spark.tar.gz \
    && tar -xvzf /tmp/spark.tar.gz -C /opt/ \
    && rm /tmp/spark.tar.gz* \
    && mv /opt/${SPARK_VERSION}-bin-hadoop3 /opt/${SPARK_VERSION}

ADD conf/core-site.xml $SPARK_HOME/conf
ADD conf/yarn-site.xml $SPARK_HOME/conf

#=========
# INSTALL PYTHON DEPS
#=========

ADD requirements.txt /requirements.txt

# run install
RUN pip install -r /requirements.txt
